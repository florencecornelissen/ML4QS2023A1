{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter7.PrepareDatasetForLearning import PrepareDatasetForLearning\n",
    "from Chapter7.Evaluation import RegressionEvaluation, ClassificationEvaluation\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalClassificationAlgorithms\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalRegressionAlgorithms\n",
    "from Chapter7.FeatureSelection import FeatureSelectionClassification\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "# from exercises_ch7_classification_individual import used_features\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "from util import util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "\n",
    "DATA_PATH = Path('./intermediate_datafiles/')\n",
    "DATASET_FNAME1 = 'chapter5_resultIvo.csv'\n",
    "DATASET_FNAME2 = 'chapter5_resultJoost.csv'\n",
    "DATASET_FNAME3 = 'chapter5_resultFlo.csv'\n",
    "# RESULT_FNAME =  'chapter3_result_outliers'+participant+'.csv'\n",
    "\n",
    "# Next, import the data from the specified location and parse the date index.\n",
    "try:\n",
    "    dataset1 = pd.read_csv(DATA_PATH / DATASET_FNAME1, index_col=0)\n",
    "    dataset2 = pd.read_csv(DATA_PATH / DATASET_FNAME2, index_col=0)\n",
    "    dataset3 = pd.read_csv(DATA_PATH / DATASET_FNAME3, index_col=0)\n",
    "except IOError as e:\n",
    "    print('File not found, try to run previous crowdsignals scripts first!')\n",
    "    raise e\n",
    "\n",
    "common_columns = set(dataset1.columns) & set(dataset2.columns) & set(dataset3.columns)\n",
    "dataset1 = dataset1[common_columns]\n",
    "dataset2 = dataset2[common_columns]\n",
    "dataset3 = dataset3[common_columns]\n",
    "\n",
    "dataset = pd.concat([dataset1, dataset2, dataset3], ignore_index=False)\n",
    "dataset.index = pd.to_datetime(dataset.index)\n",
    "\n",
    "# We'll create an instance of our visualization class to plot the results.\n",
    "DataViz = VisualizeDataset()\n",
    "\n",
    "# Of course we repeat some stuff from Chapter 3, namely to load the dataset\n",
    "\n",
    "# Read the result from the previous chapter, and make sure the index is of the type datetime.\n",
    "\n",
    "\n",
    "# Let us consider our second task, namely the prediction of the heart rate. We consider this as a temporal task.\n",
    "\n",
    "prepare = PrepareDatasetForLearning()\n",
    "\n",
    "train_X, test_X, train_y, test_y = prepare.split_single_dataset_classification(dataset, ['label'], 'like', 0.7, filter=True, temporal=True)\n",
    "train_y = pd.get_dummies(train_y)\n",
    "test_y = pd.get_dummies(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-02-08 18:28:30.860764416    1\n",
      "2016-02-08 18:28:31.260764416    1\n",
      "2016-02-08 18:28:31.660764416    1\n",
      "2016-02-08 18:28:32.060764416    1\n",
      "2016-02-08 18:28:32.460764416    1\n",
      "                                ..\n",
      "2016-02-08 18:28:56.058355968    1\n",
      "2016-02-08 18:28:56.458355968    1\n",
      "2016-02-08 18:28:56.858355968    1\n",
      "2016-02-08 18:28:57.258355968    1\n",
      "2016-02-08 18:28:57.658355968    1\n",
      "Length: 965, dtype: int64\n",
      "2016-02-08 18:28:58.058355968    1\n",
      "2016-02-08 18:28:58.458355968    1\n",
      "2016-02-08 18:28:58.858355968    1\n",
      "2016-02-08 18:28:59.258355968    1\n",
      "2016-02-08 18:28:59.658355968    1\n",
      "                                ..\n",
      "2016-02-08 18:31:42.058355968    1\n",
      "2016-02-08 18:31:42.458355968    1\n",
      "2016-02-08 18:31:42.858355968    1\n",
      "2016-02-08 18:31:43.258355968    1\n",
      "2016-02-08 18:31:43.658355968    1\n",
      "Length: 415, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_y, axis=1))\n",
    "print(np.sum(test_y, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 1s 1ms/step - loss: 1.9496 - accuracy: 0.1668\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9280 - accuracy: 0.1979\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9208 - accuracy: 0.2104\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9132 - accuracy: 0.2052\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9114 - accuracy: 0.2176\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9072 - accuracy: 0.2269\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9003 - accuracy: 0.2290\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.9063 - accuracy: 0.2073\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.8926 - accuracy: 0.2218\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.8905 - accuracy: 0.2332\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 6) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[39m.\u001b[39mfit(train_X, train_y, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_X, test_y)\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0d/s7f5w3y14tjdxlssm2bbvqjc0000gn/T/__autograph_generated_file38k98sj6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/florencecornelissen/opt/anaconda3/envs/ML4QS2023/lib/python3.8/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 6) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "import numpy as np\n",
    "\n",
    "num_classes = 7\n",
    "batch_size = 32\n",
    "feature_dim = 427\n",
    "\n",
    "# Reshape your input data to have the appropriate shape\n",
    "train_X = np.random.random([965, feature_dim]).astype(np.float32)\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))  # Reshape to (samples, timesteps, features)\n",
    "\n",
    "test_X = np.random.random([415, feature_dim]).astype(np.float32)\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))  # Reshape to (samples, timesteps, features)\n",
    "\n",
    "mmodel = Sequential()\n",
    "\n",
    "num_categories = 7\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(num_classes, activation='relu', input_shape=(None, feature_dim)))  # RNN layer\n",
    "model.add(Dense(num_categories, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X, train_y, epochs=10, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_X, test_y)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "# Assume you have your input data X and corresponding labels y ready\n",
    "\n",
    "# Step 3: Preprocess your data\n",
    "# Normalize your input data, reshape it, and one-hot encode your labels if necessary\n",
    "\n",
    "# Step 4: Split your data into training and testing sets\n",
    "# Assuming you have X_train, y_train, X_test, y_test\n",
    "\n",
    "# Step 5: Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(time_steps, num_features)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Step 6: Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 7: Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4QS2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
