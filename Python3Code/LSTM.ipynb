{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter7.PrepareDatasetForLearning import PrepareDatasetForLearning\n",
    "from Chapter7.Evaluation import RegressionEvaluation, ClassificationEvaluation\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalClassificationAlgorithms\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalRegressionAlgorithms\n",
    "from Chapter7.FeatureSelection import FeatureSelectionClassification\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "# from exercises_ch7_classification_individual import used_features\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "from util import util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x shape (90, 10, 428)\n",
      "train y shape (90, 7)\n",
      "test x shape (27, 10, 428)\n",
      "test y shape (27, 7)\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 210ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.0000\n",
      "1/1 [==============================] - 0s 147ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "\n",
    "DATA_PATH = Path('./intermediate_datafiles/')\n",
    "DATASET_FNAME1 = 'chapter5_resultIvo.csv'\n",
    "DATASET_FNAME2 = 'chapter5_resultJoost.csv'\n",
    "DATASET_FNAME3 = 'chapter5_resultFlo.csv'\n",
    "# RESULT_FNAME =  'chapter3_result_outliers'+participant+'.csv'\n",
    "\n",
    "# Next, import the data from the specified location and parse the date index.\n",
    "try:\n",
    "    dataset1 = pd.read_csv(DATA_PATH / DATASET_FNAME1, index_col=0)\n",
    "    dataset2 = pd.read_csv(DATA_PATH / DATASET_FNAME2, index_col=0)\n",
    "    dataset3 = pd.read_csv(DATA_PATH / DATASET_FNAME3, index_col=0)\n",
    "except IOError as e:\n",
    "    print('File not found, try to run previous crowdsignals scripts first!')\n",
    "    raise e\n",
    "\n",
    "common_columns = set(dataset1.columns) & set(dataset2.columns) & set(dataset3.columns)\n",
    "dataset1 = dataset1[common_columns]\n",
    "dataset2 = dataset2[common_columns]\n",
    "dataset3 = dataset3[common_columns]\n",
    "\n",
    "dataset1.index = pd.to_datetime(dataset1.index)\n",
    "dataset2.index = pd.to_datetime(dataset2.index)\n",
    "dataset3.index = pd.to_datetime(dataset3.index)\n",
    "\n",
    "# We'll create an instance of our visualization class to plot the results.\n",
    "DataViz = VisualizeDataset()\n",
    "\n",
    "# Of course we repeat some stuff from Chapter 3, namely to load the dataset\n",
    "\n",
    "# Read the result from the previous chapter, and make sure the index is of the type datetime.\n",
    "\n",
    "\n",
    "# Let us consider our second task, namely the prediction of the heart rate. We consider this as a temporal task.\n",
    "\n",
    "prepare = PrepareDatasetForLearning()\n",
    "\n",
    "train_X, test_X, train_y, test_y = prepare.split_multiple_datasets_classification([dataset1, dataset2, dataset3], ['label'], 'like', 0.7, filter=True, temporal=True, unknown_users=True)\n",
    "\n",
    "train_y_no_dummy = train_y\n",
    "test_y_no_dummy = test_y\n",
    "\n",
    "train_y = pd.get_dummies(train_y)\n",
    "test_y = pd.get_dummies(test_y)\n",
    "\n",
    "\n",
    "train_y_mode = train_y_no_dummy.iloc[::10, :].mode(axis=1)[0]\n",
    "train_y_mode = pd.get_dummies(train_y_no_dummy)\n",
    "\n",
    "test_y_mode = test_y_no_dummy.iloc[::10, :].mode(axis=1)[0]\n",
    "test_y_mode = pd.get_dummies(test_y_no_dummy)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "num_classes = train_y.shape[1]\n",
    "batch_size = 16\n",
    "feature_dim = train_X.shape[1]  # Adjusted to use the correct dimension\n",
    "\n",
    "# # Reshape your input data to have the appropriate shape\n",
    "# train_X = np.random.random([965, feature_dim]).astype(np.float32)\n",
    "\n",
    "\n",
    "# train_y = train_y_mode\n",
    "# test_y = test_y_mode\n",
    "\n",
    "\n",
    "train_X = np.reshape(train_X.values[:900], (-1, 10, train_X.shape[1]))  # Reshape to (samples, timesteps, features)\n",
    "train_y = np.reshape(train_y_mode.values[:90], (90, 7))  # Reshape to (samples, timesteps, features)\n",
    "# print(train_X)\n",
    "\n",
    "# test_X = np.random.random([415, feature_dim]).astype(np.float32)\n",
    "test_X = np.reshape(test_X.values[:270], (-1, 10, test_X.shape[1]))  # Reshape to (samples, timesteps, features)\n",
    "test_y = np.reshape(test_y_mode.values[:27], (27,7))  # Reshape to (samples, timesteps, features)\n",
    "# print(test_X)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(num_classes, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))  # RNN layer\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print('train x shape',train_X.shape)\n",
    "print('train y shape',train_y.shape)\n",
    "print('test x shape',test_X.shape)\n",
    "print('test y shape',test_y.shape)\n",
    "model.fit(train_X, train_y, epochs=10, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_X, test_y)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4QS2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
